{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Define the Problem"
      ],
      "metadata": {
        "id": "OYgCcW1qdZMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I aim to minimize inventory costs while ensuring adequate stock levels to meet demand. Our model will decide how much inventory to reorder based on current levels and forecasted demand."
      ],
      "metadata": {
        "id": "LL5rd9ldddX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Environment Setup"
      ],
      "metadata": {
        "id": "0bg5ZzoLeJse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a simplified supply chain environment where the agent decides how many units to order each period. Here’s a basic setup using a custom environment."
      ],
      "metadata": {
        "id": "abzP3qLleQ0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SupplyChainEnv:\n",
        "    def __init__(self, max_inventory=100, max_order=20, demand_mean=15, demand_std=5, holding_cost=1):\n",
        "        self.max_inventory = max_inventory\n",
        "        self.max_order = max_order\n",
        "        self.demand_mean = demand_mean\n",
        "        self.demand_std = demand_std\n",
        "        self.holding_cost = holding_cost\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.inventory = np.random.randint(0, self.max_inventory + 1)\n",
        "        return self.inventory\n",
        "\n",
        "    def step(self, action):\n",
        "        order = min(action, self.max_order)\n",
        "        demand = max(0, np.random.normal(self.demand_mean, self.demand_std))\n",
        "        self.inventory = max(0, self.inventory + order - demand)\n",
        "        cost = self.holding_cost * self.inventory\n",
        "        reward = -cost\n",
        "        done = False\n",
        "        return self.inventory, reward, done, {}\n"
      ],
      "metadata": {
        "id": "TGBI4fI9eCAP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Generate Synthetic Data"
      ],
      "metadata": {
        "id": "FTA_0DT1irQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate synthetic data to simulate historical inventory levels, orders, demand, costs, and rewards."
      ],
      "metadata": {
        "id": "n3AszsSWix8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_synthetic_data(num_periods=1000, max_inventory=100, max_order=20, demand_mean=15, demand_std=5, holding_cost=1):\n",
        "    data = {\n",
        "        'Period': [],\n",
        "        'Inventory': [],\n",
        "        'Order': [],\n",
        "        'Demand': [],\n",
        "        'Cost': [],\n",
        "        'Reward': []\n",
        "    }\n",
        "\n",
        "    inventory = np.random.randint(0, max_inventory + 1)\n",
        "\n",
        "    for period in range(num_periods):\n",
        "        order = np.random.randint(0, max_order + 1)\n",
        "        demand = max(0, np.random.normal(demand_mean, demand_std))\n",
        "        inventory = max(0, inventory + order - demand)\n",
        "        cost = holding_cost * inventory\n",
        "        reward = -cost\n",
        "\n",
        "        data['Period'].append(period)\n",
        "        data['Inventory'].append(inventory)\n",
        "        data['Order'].append(order)\n",
        "        data['Demand'].append(demand)\n",
        "        data['Cost'].append(cost)\n",
        "        data['Reward'].append(reward)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('synthetic_supply_chain_data.csv', index=False)\n",
        "    print(\"Synthetic data generated and saved as 'synthetic_supply_chain_data.csv'\")\n",
        "\n",
        "# Generate the synthetic data\n",
        "generate_synthetic_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_7fZdVMi1kR",
        "outputId": "09499f5d-21d6-4d1d-836f-c8717df0bd8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic data generated and saved as 'synthetic_supply_chain_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Q-Learning Algorithm"
      ],
      "metadata": {
        "id": "BUlP30iReaVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a simple Q-learning algorithm to train the agent. The Q-learning agent will interact with the environment to learn the optimal policy."
      ],
      "metadata": {
        "id": "UesyW5mxeeLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class DataSupplyChainEnv:\n",
        "    def __init__(self, data_file='synthetic_supply_chain_data.csv'):\n",
        "        self.data = pd.read_csv(data_file)\n",
        "        self.current_step = 0\n",
        "        self.max_steps = len(self.data)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.inventory = int(self.data.iloc[self.current_step]['Inventory'])\n",
        "        return self.inventory\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.max_steps - 1:\n",
        "            done = True\n",
        "            return self.inventory, 0, done, {}\n",
        "\n",
        "        self.current_step += 1\n",
        "        next_inventory = int(self.data.iloc[self.current_step]['Inventory'])\n",
        "        reward = self.data.iloc[self.current_step]['Reward']\n",
        "        done = self.current_step >= self.max_steps - 1\n",
        "        return next_inventory, reward, done, {}\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.99):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.q_table = np.zeros((env.max_inventory + 1, env.max_order + 1))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return np.random.randint(0, self.env.max_order + 1)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        state_idx = int(state)\n",
        "        next_state_idx = int(next_state)\n",
        "        predict = self.q_table[state_idx, action]\n",
        "        target = reward + self.discount_factor * np.max(self.q_table[next_state_idx])\n",
        "        self.q_table[state_idx, action] += self.learning_rate * (target - predict)\n",
        "\n",
        "    def train(self, episodes=1000):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                self.learn(state, action, reward, next_state)\n",
        "                state = next_state\n",
        "            self.exploration_rate *= self.exploration_decay\n"
      ],
      "metadata": {
        "id": "zZbcpmEJeWBt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update DataSupplyChainEnv Class\n",
        "Let’s adjust the DataSupplyChainEnv class to include max_inventory and max_order attributes"
      ],
      "metadata": {
        "id": "7WWvUwfUjS_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class DataSupplyChainEnv:\n",
        "    def __init__(self, data_file='synthetic_supply_chain_data.csv', max_inventory=100, max_order=20):\n",
        "        self.data = pd.read_csv(data_file)\n",
        "        self.current_step = 0\n",
        "        self.max_steps = len(self.data)\n",
        "        self.max_inventory = max_inventory\n",
        "        self.max_order = max_order\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.inventory = int(self.data.iloc[self.current_step]['Inventory'])\n",
        "        return self.inventory\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.max_steps - 1:\n",
        "            done = True\n",
        "            return self.inventory, 0, done, {}\n",
        "\n",
        "        self.current_step += 1\n",
        "        next_inventory = int(self.data.iloc[self.current_step]['Inventory'])\n",
        "        reward = self.data.iloc[self.current_step]['Reward']\n",
        "        done = self.current_step >= self.max_steps - 1\n",
        "        return next_inventory, reward, done, {}\n"
      ],
      "metadata": {
        "id": "pavn29JjjW4y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Training the Agent"
      ],
      "metadata": {
        "id": "TwzEdab_enFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the agent by letting it interact with the environment for a specified number of episodes."
      ],
      "metadata": {
        "id": "iLNHJNk1e41G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new environment using the generated data\n",
        "env = DataSupplyChainEnv(data_file='synthetic_supply_chain_data.csv')\n",
        "agent = QLearningAgent(env)\n",
        "\n",
        "# Train the agent using the data-driven environment\n",
        "agent.train(episodes=1000)\n"
      ],
      "metadata": {
        "id": "_dPTITiweil-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6: Test the Agent"
      ],
      "metadata": {
        "id": "lVtELGJqkOmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the trained agent’s performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "FwjrD-rDkRPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the agent\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = agent.choose_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "\n",
        "print(\"Total reward after testing:\", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QdmBS7pkWeA",
        "outputId": "bbbe1a09-a550-4361-fe01-f94ab16baaa2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward after testing: -3053.3465889760923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total reward of -3053.35 after testing indicates that the agent's performance might need improvement. Here are some steps to help diagnose and improve the model:\n",
        "\n",
        "1. Review the Reward Function\n",
        "Ensure that the reward function correctly reflects your goals. In this case, a negative reward represents the cost, so a lower total reward means higher costs. Make sure this aligns with your objectives.\n",
        "\n",
        "2. Check Q-Learning Parameters\n",
        "Evaluate and potentially adjust the parameters for the Q-learning algorithm:\n",
        "\n",
        "Learning Rate (learning_rate): Determines how much new information overrides old information. Typical values range from 0.01 to 0.5.\n",
        "Discount Factor (discount_factor): Balances the importance of immediate versus future rewards. Values are typically between 0.9 and 0.99.\n",
        "Exploration Rate (exploration_rate): Controls the trade-off between exploration and exploitation. It should decrease over time.\n",
        "3. Increase Training Duration\n",
        "The agent might need more training to learn optimal policies. You can increase the number of episodes.\n",
        "\n",
        "4. Check Q-Table Initialization\n",
        "Ensure that the Q-table is correctly initialized. It should be large enough to accommodate the state-action space.\n",
        "\n",
        "5. Verify Environment and Data\n",
        "Double-check that the synthetic data and environment are realistic and align with your goals. Ensure that:\n",
        "\n",
        "The data generation process creates reasonable scenarios.\n",
        "The environment's step function accurately reflects inventory changes and costs.\n",
        "6. Implement a More Advanced Algorithm\n",
        "If Q-learning is not yielding satisfactory results, consider using more advanced algorithms such as:\n",
        "\n",
        "Deep Q-Network (DQN): Useful for larger state spaces.\n",
        "Policy Gradient Methods: Directly learn a policy function.\n",
        "Revised Example Code\n",
        "Here’s how you might adjust some of these parameters and settings in the Q-learning implementation:"
      ],
      "metadata": {
        "id": "Q4f7s2I3kpXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s how you might adjust some of these parameters and settings in the Q-learning implementation:"
      ],
      "metadata": {
        "id": "ofZj5PB2lHEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.q_table = np.zeros((env.max_inventory + 1, env.max_order + 1))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            return np.random.randint(0, self.env.max_order + 1)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        state_idx = int(state)\n",
        "        next_state_idx = int(next_state)\n",
        "        predict = self.q_table[state_idx, action]\n",
        "        target = reward + self.discount_factor * np.max(self.q_table[next_state_idx])\n",
        "        self.q_table[state_idx, action] += self.learning_rate * (target - predict)\n",
        "\n",
        "    def train(self, episodes=2000):  # Increase number of episodes\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                self.learn(state, action, reward, next_state)\n",
        "                state = next_state\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "# Reinitialize and train the agent\n",
        "env = DataSupplyChainEnv(data_file='synthetic_supply_chain_data.csv')\n",
        "agent = QLearningAgent(env)\n",
        "agent.train(episodes=2000)  # Increase number of episodes\n",
        "\n",
        "# Test the agent again\n",
        "state = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = agent.choose_action(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    state = next_state\n",
        "\n",
        "print(\"Total reward after testing:\", total_reward)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxb17UEelNvE",
        "outputId": "ef0a32c5-ec23-4460-9ebc-fe59106920ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward after testing: -2564.511694439982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total reward of -2564.51 after testing represents the cumulative reward the agent accumulated over the entire testing period. In this setup, where the reward is negative and represents costs, a lower (more negative) total reward indicates higher costs incurred by the agent's actions.\n",
        "\n",
        "What Does the Result Mean?\n",
        "Higher Costs: The negative total reward suggests that the agent’s actions resulted in relatively high holding costs. This could be due to:\n",
        "\n",
        "Inefficient Inventory Management: The agent might be ordering too much or too little inventory, leading to higher costs.\n",
        "Inadequate Training: The agent may not have trained sufficiently to learn the optimal policy, especially if fewer episodes were used or if exploration was not effective.\n",
        "Evaluation of Performance:\n",
        "\n",
        "Relative Measure: The total reward value is relative and should be compared with other results or benchmarks. A lower reward indicates worse performance compared to another agent or baseline.\n",
        "Improvement Over Time: If you have historical data or multiple runs, you can compare these total rewards to assess if the agent’s performance is improving over time."
      ],
      "metadata": {
        "id": "j0UjS2X91SbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Real world Applications\n",
        "1. Retail Store Inventory Management\n",
        "Challenge: A retail store needs to manage inventory levels for a wide range of products. Overstocking leads to high holding costs and stockouts result in lost sales.\n",
        "Solution: Use reinforcement learning to optimize reorder quantities based on historical sales data, seasonal trends, and demand forecasts. The model learns to balance holding costs with the risk of stockouts.\n",
        "2. E-Commerce Fulfillment\n",
        "Challenge: E-commerce platforms often face challenges in managing stock across multiple warehouses and predicting demand spikes.\n",
        "Solution: Implement a reinforcement learning system to optimize inventory distribution across warehouses. This can help minimize shipping costs and ensure products are available where they are needed most.\n",
        "3. Manufacturing Supply Chain\n",
        "Challenge: Manufacturing companies need to manage raw materials and finished goods across various stages of production while minimizing storage costs and avoiding production delays.\n",
        "Solution: Use reinforcement learning to dynamically adjust inventory levels of raw materials and finished products. The system can help optimize procurement schedules and reduce holding costs.\n",
        "4. Grocery Store Chain\n",
        "Challenge: Managing inventory for perishable goods is critical for grocery stores. Mismanagement can lead to waste or shortages.\n",
        "Solution: Develop a reinforcement learning model to forecast demand for perishable items and optimize order quantities to reduce waste while avoiding stockouts.\n",
        "5. Pharmaceutical Supply Chain\n",
        "Challenge: Pharmaceutical companies need to manage inventory for various drugs, often with complex regulations and expiration dates.\n",
        "Solution: Reinforcement learning can help optimize inventory levels for different drugs, ensuring compliance with regulations while minimizing holding costs and avoiding shortages.\n",
        "6. Automotive Parts Distribution\n",
        "Challenge: Automotive parts distributors need to manage inventory for a wide range of parts, which vary in demand and shelf life.\n",
        "Solution: Implement a reinforcement learning-based system to optimize inventory levels and reorder points based on historical demand and lead times, reducing the risk of stockouts and overstocking.\n",
        "7. Hospital Inventory Management\n",
        "Challenge: Hospitals need to manage inventory for medical supplies, equipment, and medications, ensuring they are available when needed while minimizing costs.\n",
        "Solution: Use reinforcement learning to optimize the inventory of medical supplies and medications, taking into account usage patterns, shelf life, and emergency requirements.\n",
        "8. Consumer Electronics\n",
        "Challenge: Companies dealing with consumer electronics often experience fluctuations in demand and need to manage inventory levels for various products.\n",
        "Solution: Develop a reinforcement learning model to adjust inventory levels based on sales trends, promotions, and new product launches to optimize stock and reduce holding costs."
      ],
      "metadata": {
        "id": "LhhYiG_eemj5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9qVkJQ3lJ1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}